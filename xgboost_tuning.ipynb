{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61a40377-9154-495f-9d65-61aa0a498fbc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Generating Models for Each Airport Destination "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "675c086c-d2f7-48b8-9627-15cf694299f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ac20572-eba9-4252-aef9-d1eec4271c69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We tune the hyperparameters for the GBTRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ef1f83a-dd41-4beb-8c5d-79b578e78827",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For a more comprehsneive tuning process, we utilize k fold cross validation, with various tree sizes and maximum depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "959f1027-f0d5-4721-8646-a18ce0a6f91c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "052bf326-aa31-4d75-8ac0-eb8c7e683c1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load Spark & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370b06c1-6084-4bf0-9c79-505fc3e823cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting xgboost\n  Using cached xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl (297.1 MB)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.10/site-packages (from xgboost) (1.9.1)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.10/site-packages (from xgboost) (1.21.5)\nInstalling collected packages: xgboost\nSuccessfully installed xgboost-2.0.3\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20435f34-9933-4cc5-9137-bc3842c6a1c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41bbc111-131d-4ac8-b52f-1e5c87892c7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3535140287728786>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhyperopt\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m fmin, tpe, hp, Trials, STATUS_OK, SparkTrials\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Pipeline\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VectorAssembler\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'hyperopt'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\nFile \u001B[0;32m<command-3535140287728786>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhyperopt\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m fmin, tpe, hp, Trials, STATUS_OK, SparkTrials\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Pipeline\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VectorAssembler\n\n\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'hyperopt'",
       "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'hyperopt'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK, SparkTrials\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49478eb4-5566-4116-8a12-50c68d0b4eaa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.ml.linalg import VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae12c55b-941c-457d-b88d-cf74de74b4b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5d5f92c-160d-4f02-8510-27b7fff51a82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3535140287728788>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m spark \u001B[38;5;241m=\u001B[39m \u001B[43mSparkSession\u001B[49m\u001B[38;5;241m.\u001B[39mbuilder \\\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mairport_models\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.executor.memory\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m8g\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.driver.memory\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m4g\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.executor.cores\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.executor.instances\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m4\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;241m.\u001B[39mgetOrCreate()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'SparkSession' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-3535140287728788>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m spark \u001B[38;5;241m=\u001B[39m \u001B[43mSparkSession\u001B[49m\u001B[38;5;241m.\u001B[39mbuilder \\\n\u001B[1;32m      2\u001B[0m \u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mairport_models\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      3\u001B[0m \u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.executor.memory\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m8g\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      4\u001B[0m \u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.driver.memory\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m4g\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      5\u001B[0m \u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.executor.cores\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      6\u001B[0m \u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.executor.instances\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m4\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      7\u001B[0m \u001B[38;5;241m.\u001B[39mgetOrCreate()\n\n\u001B[0;31mNameError\u001B[0m: name 'SparkSession' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'SparkSession' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".appName(\"airport_models\") \\\n",
    ".config(\"spark.executor.memory\", \"8g\") \\\n",
    ".config(\"spark.driver.memory\", \"4g\") \\\n",
    ".config(\"spark.executor.cores\", \"2\") \\\n",
    ".config(\"spark.executor.instances\", \"4\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b9c407-b0a9-48fb-9c2f-384c39845dcc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3535140287728895>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# #create mounted directory using container and storage\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmount\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[43m  \u001B[49m\u001B[43msource\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwasbs://nguyen1@nguyen1.blob.core.windows.net\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[43m  \u001B[49m\u001B[43mmount_point\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/mnt/nguyen1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[43m  \u001B[49m\u001B[43mextra_configs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfs.azure.account.key.nguyen1.blob.core.windows.net\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[43m          \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbm9nhNqKXtqh9qi9hC005D8DiesF+tL8s2JO/b94udj2wgnxHYZwju3z6zTb0hxhxzAPaj8G5Fay+ASt+PCfEQ==\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
       "\n",
       "\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o494.mount.\n",
       ": java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/nguyen1; nested exception is: \n",
       "\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/nguyen1\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1029)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1055)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1049)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/nguyen1\n",
       "\tat scala.Predef$.require(Predef.scala:281)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:704)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1080)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:853)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1069)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:712)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:128)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:407)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:434)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:432)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:429)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:479)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:462)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:407)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:327)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:580)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:683)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:701)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:434)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:432)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:429)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:479)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:462)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:678)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:589)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:581)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:549)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:483)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:434)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:432)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:429)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:483)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:75)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:458)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:160)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.d ... (truncated)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-3535140287728895>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# #create mounted directory using container and storage\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmount\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m  \u001B[49m\u001B[43msource\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwasbs://nguyen1@nguyen1.blob.core.windows.net\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m  \u001B[49m\u001B[43mmount_point\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/mnt/nguyen1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m  \u001B[49m\u001B[43mextra_configs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfs.azure.account.key.nguyen1.blob.core.windows.net\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m          \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbm9nhNqKXtqh9qi9hC005D8DiesF+tL8s2JO/b94udj2wgnxHYZwju3z6zTb0hxhxzAPaj8G5Fay+ASt+PCfEQ==\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o494.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/nguyen1; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/nguyen1\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1029)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1055)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1049)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/nguyen1\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:704)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1080)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:853)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1069)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:712)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:128)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:407)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:434)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:432)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:429)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:479)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:462)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:407)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:327)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:580)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:683)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:701)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:434)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:432)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:429)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:479)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:462)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:589)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:581)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:549)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:483)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:434)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:432)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:429)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:483)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:75)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:458)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:160)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n\tat org.eclipse.jetty.servlet.ServletHandler.d ... (truncated)",
       "errorSummary": "java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/nguyen1; nested exception is: ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # #create mounted directory using container and storage\n",
    "# dbutils.fs.mount(\n",
    "#   source = \"wasbs://nguyen1@nguyen1.blob.core.windows.net\",\n",
    "#   mount_point = \"/mnt/nguyen1\",\n",
    "#   extra_configs = {\n",
    "#       \"fs.azure.account.key.nguyen1.blob.core.windows.net\":\n",
    "#           'bm9nhNqKXtqh9qi9hC005D8DiesF+tL8s2JO/b94udj2wgnxHYZwju3z6zTb0hxhxzAPaj8G5Fay+ASt+PCfEQ=='})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32e8d63-9a58-4d2c-8f8e-a4eb449f7cbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# REPLACE WITH PROCESSED DATA FILEPATH\n",
    "DATA_PATH = \"/mnt/nguyen1/itineraries_processed_0.1.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39b671c1-ab96-411f-b492-7ea3579c5102",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet('dbfs:/mnt/nguyen1/itineraries_processed_0.1.parquet')\n",
    "df = df.sample(fraction=0.1, withReplacement=False, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "099f426d-6ec1-4b06-8f2e-81ade64fd768",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fffc38f9-3ccb-452e-870c-3952008a982c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "200eebcf-2e8d-4eb9-8baf-a67f7a6ea7f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_columns = df.columns[:-1]\n",
    "feature_columns.remove('totalFare')\n",
    "\n",
    "# Assemble features into a vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "# df_ass = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4ee3fcf-cfcc-4e5d-ba08-ed3b222d0116",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d190cd4-6fe2-4ae2-8ae8-2694465de7aa",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:XGBoost-PySpark:Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'objective': 'reg:squarederror', 'device': 'cpu', 'numRound': 4, 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\nINFO:XGBoost-PySpark:Finished xgboost training!\n"
     ]
    }
   ],
   "source": [
    "xgbRegressor = SparkXGBRegressor(\n",
    "    features_col=\"features\", \n",
    "    label_col=\"totalFare\", \n",
    "    prediction_col=\"prediction\",\n",
    "    objective=\"reg:squarederror\",\n",
    "    numRound=4\n",
    "    #numWorkers=3  # Adjust based on your Spark setup\n",
    ")\n",
    "\n",
    "# Define the pipeline with stages\n",
    "pipeline = Pipeline(stages=[assembler, xgbRegressor])\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"totalFare\", metricName=\"rmse\")\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline_model = pipeline.fit(train_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0171c0c1-4637-4ac2-8751-19f6a774337e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 113.306\nRoot Mean Squared Error (RMSE) on test data = 113.306\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-860581649654263>, line 15\u001B[0m\n",
       "\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m predictions\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabsolute_error\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mabs\u001B[39m(col(labelCol) \u001B[38;5;241m-\u001B[39m col(predictionCol)))\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRoot Mean Squared Error (RMSE) on test data = \u001B[39m\u001B[38;5;132;01m%g\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m rmse)\n",
       "\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMean Absolute Error (MAE) on test data = \u001B[39m\u001B[38;5;132;01m%g\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m mae)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'mae' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-860581649654263>, line 15\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m predictions\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabsolute_error\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mabs\u001B[39m(col(labelCol) \u001B[38;5;241m-\u001B[39m col(predictionCol)))\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRoot Mean Squared Error (RMSE) on test data = \u001B[39m\u001B[38;5;132;01m%g\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m rmse)\n\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMean Absolute Error (MAE) on test data = \u001B[39m\u001B[38;5;132;01m%g\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m mae)\n\n\u001B[0;31mNameError\u001B[0m: name 'mae' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'mae' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = pipeline_model.transform(test_data)\n",
    "\n",
    "# Evaluate the best model\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b539e4e1-aa4f-40ec-8ca2-389e280bfdb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE) on test data = 70.0779\n"
     ]
    }
   ],
   "source": [
    "evaluator.setMetricName(\"mae\")\n",
    "mae = evaluator.evaluate(predictions)\n",
    "print(\"Mean Absolute Error (MAE) on test data = %g\" % mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf0a275e-4aa7-47e6-96d1-3c9643a73867",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## XGboost Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de7f3c44-d6a9-4425-8b93-ca6847c1f4cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 06:43:20,789 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 10}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:43:52,845 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:44:29,498 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 15}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:45:05,805 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:45:12,925 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 20}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:45:49,266 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:45:56,179 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 10}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:46:29,596 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:46:36,443 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 15}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:47:12,482 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:47:19,513 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 20}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:48:10,032 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:48:17,443 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 10}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:48:52,946 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:48:59,888 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 15}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:49:41,000 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:49:47,985 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 20}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:50:30,494 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:50:38,148 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 10}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:51:51,304 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:52:21,044 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 15}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:52:55,791 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:53:02,644 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 20}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:53:39,343 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:53:46,054 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 10}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:54:19,622 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:54:26,386 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 15}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:55:02,663 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:55:09,480 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 20}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:55:48,246 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:55:55,379 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 10}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:56:31,320 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:56:37,971 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 15}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:57:16,913 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:57:23,767 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 20}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:58:06,501 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 06:58:19,322 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 10}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 06:59:57,772 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 07:00:44,720 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 15}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 07:01:22,626 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 07:01:30,166 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 3, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 20}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 07:02:06,169 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 07:02:13,513 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 10}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 07:02:48,413 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 07:02:55,721 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 15}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 07:03:33,328 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 07:03:40,622 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 20}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 07:04:20,203 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 07:04:27,727 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 10}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 07:05:04,193 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 07:05:11,623 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 15}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 07:05:51,912 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 07:05:59,254 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 20}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 07:06:49,411 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2024-04-12 07:07:03,083 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n\tbooster params: {'device': 'cpu', 'max_depth': 7, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 20}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2024-04-12 07:09:21,444 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+---------+---------+------------------+---+-----------------------+--------------------------+--------+--------+--------------------+---------+--------------+-----------------+------------+---------------+--------------------+------------------+\n|isBasicEconomy|isRefundable|isNonStop|totalFare|days_before_flight|day|startingAirport_encoded|destinationAirport_encoded|num_legs|All_Same|airline_name_encoded| distance|departure_hour|departure_dow_idx|starting_pop|destination_pop|            features|        prediction|\n+--------------+------------+---------+---------+------------------+---+-----------------------+--------------------------+--------+--------+--------------------+---------+--------------+-----------------+------------+---------------+--------------------+------------------+\n|             0|           0|        0|    43.08|                16|  3|         (15,[6],[1.0])|            (15,[8],[1.0])|       2|       1|      (13,[4],[1.0])| 1049.121|            13|              6.0|         0.0|      20.139475|(54,[3,4,11,28,35...|193.63577270507812|\n|             0|           0|        0|    50.68|                 4| 24|         (15,[9],[1.0])|            (15,[8],[1.0])|       2|       1|      (13,[4],[1.0])| 1633.855|            17|              0.0|   20.764748|      20.139475|(54,[3,4,14,28,35...|236.32437133789062|\n|             0|           0|        0|    50.68|                 6| 24|         (15,[9],[1.0])|            (15,[8],[1.0])|       2|       1|      (13,[4],[1.0])| 1633.855|            11|              0.0|   20.764748|      20.139475|(54,[3,4,14,28,35...|227.10910034179688|\n|             0|           0|        0|    50.68|                 8|  7|         (15,[8],[1.0])|            (15,[9],[1.0])|       2|       1|      (13,[4],[1.0])| 1633.855|            21|              0.0|   20.139475|      20.764748|(54,[3,4,13,29,35...| 145.7843017578125|\n|             0|           0|        0|    50.68|                 9|  7|         (15,[9],[1.0])|            (15,[8],[1.0])|       2|       1|      (13,[4],[1.0])| 1633.855|            14|              0.0|   20.764748|      20.139475|(54,[3,4,14,28,35...|184.22622680664062|\n|             0|           0|        0|    50.68|                 9| 17|         (15,[9],[1.0])|            (15,[8],[1.0])|       2|       1|      (13,[4],[1.0])| 1633.855|            11|              0.0|   20.764748|      20.139475|(54,[3,4,14,28,35...|184.68943786621094|\n|             0|           0|        0|    50.68|                 9| 23|         (15,[9],[1.0])|            (15,[8],[1.0])|       2|       1|      (13,[4],[1.0])| 1633.855|            17|              3.0|   20.764748|      20.139475|(54,[3,4,14,28,35...| 232.5627899169922|\n|             0|           0|        0|    50.68|                11|  7|         (15,[9],[1.0])|            (15,[8],[1.0])|       2|       1|      (13,[4],[1.0])| 1633.855|            14|              0.0|   20.764748|      20.139475|(54,[3,4,14,28,35...|173.34304809570312|\n|             0|           0|        0|    50.68|                11| 24|         (15,[9],[1.0])|            (15,[8],[1.0])|       2|       1|      (13,[4],[1.0])| 1633.855|            11|              0.0|   20.764748|      20.139475|(54,[3,4,14,28,35...|173.80625915527344|\n|             0|           0|        0|    50.68|                12|  3|         (15,[2],[1.0])|            (15,[8],[1.0])|       2|       1|      (13,[4],[1.0])| 2027.827|            16|              0.0|    13.12786|      20.139475|(54,[3,4,7,28,35,...|150.14483642578125|\n|             0|           0|        0|    50.68|                12|  7|        (15,[13],[1.0])|            (15,[8],[1.0])|       2|       1|      (13,[4],[1.0])|1749.7172|             9|              0.0|       100.0|      20.139475|(54,[3,4,18,28,35...| 176.4688262939453|\n|             0|           0|        0|    50.68|                13|  3|        (15,[13],[1.0])|            (15,[8],[1.0])|       2|       1|      (13,[4],[1.0])|1749.7172|             9|              0.0|       100.0|      20.139475|(54,[3,4,18,28,35...| 167.3168182373047|\n|             0|           0|        0|    50.68|                13|  7|         (15,[8],[1.0])|            (15,[9],[1.0])|       2|       1|      (13,[4],[1.0])| 1633.855|            21|              0.0|   20.139475|      20.764748|(54,[3,4,13,29,35...|  134.901123046875|\n|             0|           0|        0|    50.68|                13|  7|        (15,[13],[1.0])|            (15,[8],[1.0])|       2|       1|      (13,[4],[1.0])|1749.7172|             6|              0.0|       100.0|      20.139475|(54,[3,4,18,28,35...|172.49122619628906|\n|             0|           0|        0|    50.68|                13| 18|        (15,[13],[1.0])|            (15,[8],[1.0])|       2|       1|      (13,[4],[1.0])|1749.7172|             9|              1.0|       100.0|      20.139475|(54,[3,4,18,28,35...|178.78912353515625|\n|             0|           0|        0|    50.68|                13| 19|         (15,[8],[1.0])|            (15,[9],[1.0])|       2|       1|      (13,[4],[1.0])| 1633.855|             6|              2.0|   20.139475|      20.764748|(54,[3,4,13,29,35...| 172.9272918701172|\n|             0|           0|        0|    50.68|                14|  4|         (15,[0],[1.0])|           (15,[14],[1.0])|       2|       1|      (13,[4],[1.0])| 542.8095|            13|              1.0|    61.52047|      11.999253|(54,[3,4,5,34,35,...|101.36678314208984|\n|             0|           0|        0|    50.68|                14| 19|         (15,[8],[1.0])|            (15,[9],[1.0])|       2|       1|      (13,[4],[1.0])| 1633.855|             6|              2.0|   20.139475|      20.764748|(54,[3,4,13,29,35...|168.93553161621094|\n|             0|           0|        0|    50.68|                14| 20|        (15,[14],[1.0])|            (15,[0],[1.0])|       2|       1|      (13,[4],[1.0])| 542.8095|            22|              3.0|   11.999253|       61.52047|(54,[3,4,19,20,35...| 173.4364013671875|\n|             0|           0|        0|    50.68|                15|  6|         (15,[8],[1.0])|            (15,[9],[1.0])|       2|       1|      (13,[4],[1.0])| 1633.855|            21|              3.0|   20.139475|      20.764748|(54,[3,4,13,29,35...|   188.41455078125|\n+--------------+------------+---------+---------+------------------+---+-----------------------+--------------------------+--------+--------+--------------------+---------+--------------+-----------------+------------+---------------+--------------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "xgb_regressor = SparkXGBRegressor(\n",
    "    features_col=\"features\", \n",
    "    label_col=\"totalFare\", \n",
    "    prediction_col=\"prediction\",\n",
    "    objective=\"reg:squarederror\",\n",
    ")\n",
    "\n",
    "# Define the pipeline with stages\n",
    "pipeline = Pipeline(stages=[assembler, xgb_regressor])\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(xgb_regressor.max_depth, [3, 5, 7, 9, 10]) \\\n",
    "    .addGrid(xgb_regressor.n_estimators, [10, 15, 20, 40, 30]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=RegressionEvaluator(\n",
    "        labelCol=\"totalFare\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"rmse\"\n",
    "    ),\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "prediction = cv_model.transform(test_data)\n",
    "prediction.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "779f67f8-ae16-44cb-85bd-6cdd157cf031",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 120.632\nMean Absolute Error (MAE) on test data = 76.205\n"
     ]
    }
   ],
   "source": [
    "# Define evaluator\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"totalFare\", metricName=\"rmse\")\n",
    "\n",
    "# Evaluate the best model\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "evaluator.setMetricName(\"mae\")\n",
    "mae = evaluator.evaluate(prediction)\n",
    "print(\"Mean Absolute Error (MAE) on test data = %g\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed66716-ae87-4434-8fe6-d4a750fb5d6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Parameters:\nenable_sparse_data_optim : False\nfeaturesCol : features\nfeatures_cols : []\nlabelCol : totalFare\npredictionCol : prediction\narbitrary_params_dict : {}\nbase_score : None\nbooster : None\ncallbacks : None\ncolsample_bylevel : None\ncolsample_bynode : None\ncolsample_bytree : None\ndevice : cpu\nearly_stopping_rounds : None\neval_metric : None\nfeature_names : None\nfeature_types : None\nfeature_weights : None\nforce_repartition : False\ngamma : None\ngrow_policy : None\nimportance_type : None\ninteraction_constraints : None\niteration_range : None\nlearning_rate : None\nmax_bin : None\nmax_cat_threshold : None\nmax_cat_to_onehot : None\nmax_delta_step : None\nmax_depth : 7\nmax_leaves : None\nmin_child_weight : None\nmissing : nan\nmonotone_constraints : None\nmulti_strategy : None\nn_estimators : 20\nnum_parallel_tree : None\nnum_workers : 1\nobjective : reg:squarederror\nrandom_state : None\nreg_alpha : None\nreg_lambda : None\nrepartition_random_shuffle : False\nsampling_method : None\nscale_pos_weight : None\nsubsample : None\ntree_method : None\nuse_gpu : False\nvalidate_parameters : None\nverbose : True\nverbosity : None\nxgb_model : None\n"
     ]
    }
   ],
   "source": [
    "# Get the best model\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Access the stages of the pipeline\n",
    "stages = best_model.stages\n",
    "\n",
    "# Access the parameters of the RandomForestRegressor stage\n",
    "rf_params = stages[-1].extractParamMap()\n",
    "\n",
    "# Print the parameters\n",
    "print(\"Best Model Parameters:\")\n",
    "for param, value in rf_params.items():\n",
    "    print(param.name, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dcb8a35-31c1-489b-8ec8-ffa2da7300ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data: 120.63204640429475\nMAE on test data: 76.20495934636185\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create an evaluator for RMSE and MAE\n",
    "evaluatorRMSE = RegressionEvaluator(\n",
    "    labelCol=\"totalFare\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "evaluatorMAE = RegressionEvaluator(\n",
    "    labelCol=\"totalFare\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "# Evaluate the best model on a test set if you have separated one\n",
    "# Assuming 'testData' is available\n",
    "predictions = best_model.transform(test_data)\n",
    "\n",
    "# Calculate RMSE and MAE\n",
    "rmse = evaluatorRMSE.evaluate(predictions)\n",
    "mae = evaluatorMAE.evaluate(predictions)\n",
    "print(\"RMSE on test data:\", rmse)\n",
    "print(\"MAE on test data:\", mae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9498f58d-bec4-4258-8cb5-f0dc5be5bf62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 3, Num Rounds: 10, RMSE: 137.97848986734505\nMax Depth: 3, Num Rounds: 15, RMSE: 134.32959671717995\nMax Depth: 3, Num Rounds: 20, RMSE: 132.60990374977555\nMax Depth: 5, Num Rounds: 10, RMSE: 131.45368530305385\nMax Depth: 5, Num Rounds: 15, RMSE: 128.41338851141504\nMax Depth: 5, Num Rounds: 20, RMSE: 126.58390977948856\nMax Depth: 7, Num Rounds: 10, RMSE: 126.06244793316499\nMax Depth: 7, Num Rounds: 15, RMSE: 122.84104203360154\nMax Depth: 7, Num Rounds: 20, RMSE: 120.57143460060239\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'cvModel' is the fitted CrossValidator model\n",
    "avg_metrics = cv_model.avgMetrics  # This holds the average RMSE for each combination\n",
    "\n",
    "# Iterate over each combination of parameters and their corresponding metric\n",
    "for i, params in enumerate(paramGrid):\n",
    "    # Extract the specific parameters\n",
    "    max_depth = params[xgb_regressor.max_depth]\n",
    "    num_round = params[xgb_regressor.n_estimators]\n",
    "    rmse = avg_metrics[i]  # Assuming RMSE is the metric used in CrossValidator setup\n",
    "\n",
    "    # Print each combination with its RMSE\n",
    "    print(f\"Max Depth: {max_depth}, Num Rounds: {num_round}, RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f36e9177-b65f-423a-972c-e7ecf6193e42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the model to a specified path\n",
    "modelPath = \"/mnt/nguyen1/xgb_model_small\"\n",
    "cv_model.save(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dee205a9-605c-4ad5-bb06-90d61d7b1698",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a4ce1c-a150-4f56-b19e-04155503e2bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ac98dec-804e-4381-ad75-1e891825459f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "airport_models",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
