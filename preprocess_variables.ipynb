{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a994ef-1a5a-4901-8caf-f373ed93d751",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07324ff-5151-4daf-9209-d53bf5dec2fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e95c35-c982-4690-b450-57cc3ea6f1af",
   "metadata": {},
   "source": [
    "In this notebook, we will edit the data to include only columns that we want. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402218af-f624-4489-85b9-9a67f2af19ff",
   "metadata": {},
   "source": [
    "There are two types of variables that we want to keep:\n",
    "- Those which are practical for the user to input when looking up a price prediction\n",
    "- Those which would actually contribute to the price of a flight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c6435-0dcc-4b8f-b6c2-146b8e698d20",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f043b592-70e0-4c63-8a73-83386027bd88",
   "metadata": {},
   "source": [
    "## Data Dictionary Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5faef-4eed-4614-ac9d-37813c657af1",
   "metadata": {},
   "source": [
    "below is a reminder of what variables we have, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d702f5e-a702-455b-9906-0a70e3ad0664",
   "metadata": {},
   "source": [
    "- legId: An identifier for the flight. // Not needed\n",
    "- searchDate: The date (YYYY-MM-DD) on which this entry was taken from Expedia. // Change search date and flight date into one variable - days before flight\n",
    "- flightDate: The date (YYYY-MM-DD) of the flight. // Potentially separate this into just the day. Month is meaningless as the data is not even for a full year\n",
    "- startingAirport: Three-character IATA airport code for the initial location. // One hot encoding\n",
    "- destinationAirport: Three-character IATA airport code for the arrival location. // One hot encoding\n",
    "- fareBasisCode: The fare basis code. // Not required\n",
    "- travelDuration: The travel duration in hours and minutes. // Not required\n",
    "- elapsedDays: The number of elapsed days (usually 0). // Not required\n",
    "- isBasicEconomy: Boolean for whether the ticket is for basic economy. // Change into binary\n",
    "- isRefundable: Boolean for whether the ticket is refundable. // Change into binary \n",
    "- isNonStop: Boolean for whether the flight is non-stop. // Change into binary \n",
    "- baseFare: The price of the ticket (in USD). // Not required\n",
    "- totalFare: The price of the ticket (in USD) including taxes and other fees. // No change needed\n",
    "- seatsRemaining: Integer for the number of seats remaining. // Not required\n",
    "- totalTravelDistance: The total travel distance in miles. This data is sometimes missing. // Not required\n",
    "- segmentsDepartureTimeEpochSeconds: String containing the departure time (Unix time) for each leg of the trip. The entries for each of the legs are separated by '||'. // Not required\n",
    "- segmentsDepartureTimeRaw: String containing the departure time (ISO 8601 format: YYYY-MM-DDThh:mm:ss.000±[hh]:00) for each leg of the trip. The entries for each of the legs are separated by '||'. // Not required\n",
    "- segmentsArrivalTimeEpochSeconds: String containing the arrival time (Unix time) for each leg of the trip. The entries for each of the legs are separated by '||'. // Not required\n",
    "- segmentsArrivalTimeRaw: String containing the arrival time (ISO 8601 format: YYYY-MM-DDThh:mm:ss.000±[hh]:00) for each leg of the trip. The entries for each of the legs are separated by '||'. // Not required\n",
    "- segmentsArrivalAirportCode: String containing the IATA airport code for the arrival location for each leg of the trip. The entries for each of the legs are separated by '||'. // Not required\n",
    "- segmentsDepartureAirportCode: String containing the IATA airport code for the departure location for each leg of the trip. The entries for each of the legs are separated by '||'. // Not required\n",
    "- segmentsAirlineName: String containing the name of the airline that services each leg of the trip. The entries for each of the legs are separated by '||'. // Most trips use the same airlines the whole way, drop trips that don't do this and change trips that do into one hot encoding. We can also use this to count how many layovers there are.\n",
    "- segmentsAirlineCode: String containing the two-letter airline code that services each leg of the trip. The entries for each of the legs are separated by '||'. // Not required\n",
    "- segmentsEquipmentDescription: String containing the type of airplane used for each leg of the trip (e.g. \"Airbus A321\" or \"Boeing 737-800\"). The entries for each of the legs are separated by '||'. // Not required\n",
    "- segmentsDurationInSeconds: String containing the duration of the flight (in seconds) for each leg of the trip. The entries for each of the legs are separated by '||'. // Not required\n",
    "- segmentsDistance: String containing the distance traveled (in miles) for each leg of the trip. The entries for each of the legs are separated by '||'. // Not required\n",
    "- segmentsCabinCode: String containing the cabin for each leg of the trip (e.g. \"coach\"). The entries for each of the legs are separated by '||'. // Not required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30d748-2989-44e4-9020-58197f0c0821",
   "metadata": {},
   "source": [
    "## Load Spark and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "efb444d5-e232-4a68-a390-837922fd9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, datediff, dayofmonth, when, udf, split\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.types import IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dbcd0942-a36b-4c84-93e2-fac0cc80d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"flights\").getOrCreate()\n",
    "\n",
    "# REPLACE WITH DATA FILEPATH\n",
    "DATA_PATH = \"../data/itineraries.csv\"\n",
    "\n",
    "df = spark.read.csv(DATA_PATH, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa7afd-e41c-4471-a124-8a0e5389c324",
   "metadata": {},
   "source": [
    "## Drop the Uneccessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "526cf67d-c9d8-4547-a009-ecb1c03f706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"legId\", \"fareBasisCode\", \"travelDuration\", \"elapsedDays\", \"baseFare\", \"seatsRemaining\", \"totalTravelDistance\",\\\n",
    "                \"segmentsDepartureTimeEpochSeconds\",\"segmentsDepartureTimeRaw\", \"segmentsArrivalTimeEpochSeconds\", \"segmentsArrivalTimeRaw\",\\\n",
    "                \"segmentsArrivalAirportCode\", \"segmentsDepartureAirportCode\",\"segmentsAirlineCode\", \"segmentsEquipmentDescription\", \\\n",
    "                \"segmentsDurationInSeconds\", \"segmentsDistance\", \"segmentsCabinCode\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8ba82933-640a-4b01-a58b-ec48ed658eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cols\n",
    "df = df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2e8f4bc2-4b40-43d7-8561-c6f5cd5138f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------------+------------------+--------------+------------+---------+---------+--------------------+\n",
      "|searchDate|flightDate|startingAirport|destinationAirport|isBasicEconomy|isRefundable|isNonStop|totalFare| segmentsAirlineName|\n",
      "+----------+----------+---------------+------------------+--------------+------------+---------+---------+--------------------+\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|     true|    248.6|               Delta|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|     true|    248.6|               Delta|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|     true|    248.6|               Delta|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|     true|    248.6|               Delta|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|     true|    248.6|               Delta|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|     true|    248.6|     JetBlue Airways|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|    false|    251.1|American Airlines...|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|    false|    251.1|American Airlines...|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|    false|    251.1|American Airlines...|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|    false|    251.1|American Airlines...|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|    false|    252.6|      United||United|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|    false|    252.6|American Airlines...|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|    false|    252.6|American Airlines...|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|    false|    252.6|American Airlines...|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|    false|    252.6|      United||United|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|    false|   290.58|Spirit Airlines||...|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|     true|    300.1|   American Airlines|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|    false|   302.11|American Airlines...|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|    false|   302.11|American Airlines...|\n",
      "|2022-04-16|2022-04-17|            ATL|               BOS|         false|       false|    false|   302.11|American Airlines...|\n",
      "+----------+----------+---------------+------------------+--------------+------------+---------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386fe086-6d9c-4e1b-b312-e41f2cc87358",
   "metadata": {},
   "source": [
    "## Days before flight Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e38dcd9c-4eb2-4c5b-9847-25248371c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"days_before_flight\", datediff(col(\"flightDate\"), col(\"searchDate\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f35deb-ad31-4083-a3d0-cccb2f99f08a",
   "metadata": {},
   "source": [
    "## DayofMonth Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "efcd6fed-7a29-4479-8f9a-f89f41e4560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"day\", dayofmonth(col(\"flightDate\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c815be-6f04-4102-b009-dd94ceb623f1",
   "metadata": {},
   "source": [
    "## Drop: searchDay and FlightDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "67f1b049-b988-45e3-865e-52bbd2acc3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*[\"searchDate\", \"flightDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d7c502d7-7c04-4b9e-874d-55ac9a7b66a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+--------------+------------+---------+---------+--------------------+------------------+---+\n",
      "|startingAirport|destinationAirport|isBasicEconomy|isRefundable|isNonStop|totalFare| segmentsAirlineName|days_before_flight|day|\n",
      "+---------------+------------------+--------------+------------+---------+---------+--------------------+------------------+---+\n",
      "|            ATL|               BOS|         false|       false|     true|    248.6|               Delta|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|     true|    248.6|               Delta|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|     true|    248.6|               Delta|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|     true|    248.6|               Delta|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|     true|    248.6|               Delta|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|     true|    248.6|     JetBlue Airways|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|    false|    251.1|American Airlines...|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|    false|    251.1|American Airlines...|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|    false|    251.1|American Airlines...|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|    false|    251.1|American Airlines...|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|    false|    252.6|      United||United|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|    false|    252.6|American Airlines...|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|    false|    252.6|American Airlines...|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|    false|    252.6|American Airlines...|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|    false|    252.6|      United||United|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|    false|   290.58|Spirit Airlines||...|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|     true|    300.1|   American Airlines|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|    false|   302.11|American Airlines...|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|    false|   302.11|American Airlines...|                 1| 17|\n",
      "|            ATL|               BOS|         false|       false|    false|   302.11|American Airlines...|                 1| 17|\n",
      "+---------------+------------------+--------------+------------+---------+---------+--------------------+------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501df872-a87d-4cc6-9f57-8a8d366e0fd1",
   "metadata": {},
   "source": [
    "## One Hot Encode Starting and Destination Airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7e7048e-1f9c-438f-83b9-15d079bb105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string indexers\n",
    "string_indexer = StringIndexer(inputCol=\"startingAirport\", outputCol=\"startingAirport_index\")\n",
    "string_indexer_2 = StringIndexer(inputCol=\"destinationAirport\", outputCol=\"destinationAirport_index\")\n",
    "\n",
    "# encoders\n",
    "encoder = OneHotEncoder(inputCol=\"startingAirport_index\", outputCol=\"startingAirport_encoded\")\n",
    "encoder_2 = OneHotEncoder(inputCol=\"destinationAirport_index\", outputCol=\"destinationAirport_encoded\")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(stages=[string_indexer, string_indexer_2, encoder, encoder_2])\n",
    "pipeline_model = pipeline.fit(df)\n",
    "df = pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e0330-455c-4b32-a552-2ffb0312def4",
   "metadata": {},
   "source": [
    "## Drop: startingAirport, destinationAirport, startingAirport_index, destinationAirport_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10c5b3ad-9e0b-4245-92f0-e5fb17b16150",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*[\"startingAirport\", \"destinationAirport\", \"startingAirport_index\", \"destinationAirport_index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d193f-1c64-4118-b1b3-8bcacf95f972",
   "metadata": {},
   "source": [
    "## Convert Boolean Columns into Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2644363b-404d-4ecf-a4cb-2dc9a928798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_columns = [\"isBasicEconomy\", \"isRefundable\", \"isNonStop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9ee1befa-abf9-4c47-a263-e9013c772741",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in bool_columns:\n",
    "    df = df.withColumn(col_name, when(col(col_name), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a4c8f540-2654-4a05-b390-d74523aa2c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+---------+---------+--------------------+------------------+---+-----------------------+--------------------------+\n",
      "|isBasicEconomy|isRefundable|isNonStop|totalFare| segmentsAirlineName|days_before_flight|day|startingAirport_encoded|destinationAirport_encoded|\n",
      "+--------------+------------+---------+---------+--------------------+------------------+---+-----------------------+--------------------------+\n",
      "|             0|           0|        1|    248.6|               Delta|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        1|    248.6|               Delta|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        1|    248.6|               Delta|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        1|    248.6|               Delta|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        1|    248.6|               Delta|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        1|    248.6|     JetBlue Airways|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    251.1|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    251.1|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    251.1|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    251.1|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    252.6|      United||United|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    252.6|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    252.6|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    252.6|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    252.6|      United||United|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|   290.58|Spirit Airlines||...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        1|    300.1|   American Airlines|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|   302.11|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|   302.11|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|   302.11|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "+--------------+------------+---------+---------+--------------------+------------------+---+-----------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5807c7d0-876a-4af8-95b5-4e2cecfdd265",
   "metadata": {},
   "source": [
    "## Create Number of Layovers and Airline Name\n",
    "## TODO: FIX THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c7abb4bf-6bf9-4a66-9633-4274ee41d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_equal(iterable):\n",
    "    g = groupby(iterable)\n",
    "    return next(g, True) and not next(g, False)\n",
    "\n",
    "# get airline name\n",
    "def get_airline(segmentsAirlineName):\n",
    "    splits = segmentsAirlineName.split(\"||\")\n",
    "    if all_equal(splits):\n",
    "        return splits[0]\n",
    "    else:\n",
    "        return \"Multi-Airline\"\n",
    "\n",
    "# get number of splits\n",
    "def get_nsplits(segmentsAirlineName):\n",
    "    splits = segmentsAirlineName.split(\"||\")\n",
    "    return len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "591dce45-a4c6-4f70-b608-bd741fde89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register as udf \n",
    "airline_udf = udf(get_airline)\n",
    "nsplit_udf = udf(get_nsplits, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4bbd3-b2ac-44c1-941d-0c1d4d98d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"age_plus_5\", col(\"segmentsAirlineName\") + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "49284d53-83c8-40d6-a9f7-4c5ea5910a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.select(\"segmentsAirlineName\").limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e1f8a5e2-588e-4313-bf6b-05a026624dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/19 16:24:20 ERROR Executor: Exception in task 0.0 in stage 95.0 (TID 81)1]\n",
      "org.apache.spark.SparkException: \n",
      "Bad data in pyspark.daemon's standard output. Invalid port number:\n",
      "  1231975525 (0x496e7465)\n",
      "Python command to execute the daemon was:\n",
      "  python3 -m pyspark.daemon\n",
      "Check that you don't have any unexpected modules or libraries in\n",
      "your PYTHONPATH:\n",
      "  /Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip:/Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.1.jar\n",
      "Also, check if you have a sitecustomize.py module in your python path,\n",
      "or in your python installation, that is printing to standard output\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/03/19 16:24:20 WARN TaskSetManager: Lost task 0.0 in stage 95.0 (TID 81) (192.168.110.41 executor driver): org.apache.spark.SparkException: \n",
      "Bad data in pyspark.daemon's standard output. Invalid port number:\n",
      "  1231975525 (0x496e7465)\n",
      "Python command to execute the daemon was:\n",
      "  python3 -m pyspark.daemon\n",
      "Check that you don't have any unexpected modules or libraries in\n",
      "your PYTHONPATH:\n",
      "  /Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip:/Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.1.jar\n",
      "Also, check if you have a sitecustomize.py module in your python path,\n",
      "or in your python installation, that is printing to standard output\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/19 16:24:20 ERROR TaskSetManager: Task 0 in stage 95.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1608.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 95.0 failed 1 times, most recent failure: Lost task 0.0 in stage 95.0 (TID 81) (192.168.110.41 executor driver): org.apache.spark.SparkException: \nBad data in pyspark.daemon's standard output. Invalid port number:\n  1231975525 (0x496e7465)\nPython command to execute the daemon was:\n  python3 -m pyspark.daemon\nCheck that you don't have any unexpected modules or libraries in\nyour PYTHONPATH:\n  /Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip:/Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.1.jar\nAlso, check if you have a sitecustomize.py module in your python path,\nor in your python installation, that is printing to standard output\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: \nBad data in pyspark.daemon's standard output. Invalid port number:\n  1231975525 (0x496e7465)\nPython command to execute the daemon was:\n  python3 -m pyspark.daemon\nCheck that you don't have any unexpected modules or libraries in\nyour PYTHONPATH:\n  /Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip:/Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.1.jar\nAlso, check if you have a sitecustomize.py module in your python path,\nor in your python installation, that is printing to standard output\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#df.withColumn(\"airline_name\", airline_udf(df[\"segmentsAirlineName\"])).show()\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mairline_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mairline_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msegmentsAirlineName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#df.withColumn(\"nStops\", nsplit_udf(df[\"segmentsAirlineName\"])).show()\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1608.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 95.0 failed 1 times, most recent failure: Lost task 0.0 in stage 95.0 (TID 81) (192.168.110.41 executor driver): org.apache.spark.SparkException: \nBad data in pyspark.daemon's standard output. Invalid port number:\n  1231975525 (0x496e7465)\nPython command to execute the daemon was:\n  python3 -m pyspark.daemon\nCheck that you don't have any unexpected modules or libraries in\nyour PYTHONPATH:\n  /Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip:/Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.1.jar\nAlso, check if you have a sitecustomize.py module in your python path,\nor in your python installation, that is printing to standard output\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: \nBad data in pyspark.daemon's standard output. Invalid port number:\n  1231975525 (0x496e7465)\nPython command to execute the daemon was:\n  python3 -m pyspark.daemon\nCheck that you don't have any unexpected modules or libraries in\nyour PYTHONPATH:\n  /Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip:/Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/Users/eddie/opt/anaconda3/envs/bigdata/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.1.jar\nAlso, check if you have a sitecustomize.py module in your python path,\nor in your python installation, that is printing to standard output\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "test_df.withColumn(\"airline_name\", airline_udf(col(\"segmentsAirlineName\"))).show()\n",
    "\n",
    "# this shit aint working\n",
    "#df.withColumn(\"airline_name\", airline_udf(df[\"segmentsAirlineName\"])).show()\n",
    "#df.withColumn(\"nStops\", nsplit_udf(df[\"segmentsAirlineName\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977abb31-db4d-4678-b052-f3e455bacf37",
   "metadata": {},
   "source": [
    "## Save the final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f95a6-8978-40dc-b08c-26177755689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the file should be saved\n",
    "TARGET_PATH = \"../data/itineraries_processed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c3021438-9860-4dca-83d8-87d06d976aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+---------+---------+--------------------+------------------+---+-----------------------+--------------------------+\n",
      "|isBasicEconomy|isRefundable|isNonStop|totalFare| segmentsAirlineName|days_before_flight|day|startingAirport_encoded|destinationAirport_encoded|\n",
      "+--------------+------------+---------+---------+--------------------+------------------+---+-----------------------+--------------------------+\n",
      "|             0|           0|        1|    248.6|               Delta|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        1|    248.6|               Delta|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        1|    248.6|               Delta|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        1|    248.6|               Delta|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        1|    248.6|               Delta|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        1|    248.6|     JetBlue Airways|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    251.1|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    251.1|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    251.1|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    251.1|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    252.6|      United||United|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    252.6|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    252.6|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    252.6|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|    252.6|      United||United|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|   290.58|Spirit Airlines||...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        1|    300.1|   American Airlines|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|   302.11|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|   302.11|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "|             0|           0|        0|   302.11|American Airlines...|                 1| 17|         (15,[1],[1.0])|            (15,[2],[1.0])|\n",
      "+--------------+------------+---------+---------+--------------------+------------------+---+-----------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.csv(TARGET_PATH, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f141dd-de83-48d8-94c9-d55484ca6a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
